
=> Loading DataSet with WebDataset using .tars
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Using native Torch DistributedDataParallel.
iter per epoch: 2502
Scheduled iters: 250200
Scheduled epochs: 100
dataset_size:1281167, batch_size:64, world_size(Total Devices):8
----> Number of batches to be processed per CPU = 2502
Train: 0 [   0/2502 (  0%)]  Loss:  6.950428 (6.9504)  Time: 7.543s,   67.88/s  (7.543s,   67.88/s)  LR: 1.000e-04  Data: 5.484 (5.484)
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/2502 (  2%)]  Loss:  6.893866 (6.9221)  Time: 4.901s,  104.46/s  (5.186s,   98.72/s)  LR: 1.020e-04  Data: 3.582 (3.595)
Train: 0 [ 100/2502 (  4%)]  Loss:  6.891633 (6.9120)  Time: 5.409s,   94.65/s  (5.165s,   99.12/s)  LR: 1.040e-04  Data: 3.253 (3.601)
Train: 0 [ 150/2502 (  6%)]  Loss:  6.856450 (6.8981)  Time: 5.087s,  100.66/s  (5.157s,   99.28/s)  LR: 1.060e-04  Data: 3.908 (3.604)
Train: 0 [ 200/2502 (  8%)]  Loss:  6.826077 (6.8837)  Time: 5.301s,   96.59/s  (5.161s,   99.21/s)  LR: 1.080e-04  Data: 3.697 (3.615)
Train: 0 [ 250/2502 ( 10%)]  Loss:  6.789088 (6.8679)  Time: 5.417s,   94.53/s  (5.175s,   98.93/s)  LR: 1.100e-04  Data: 3.797 (3.623)
Train: 0 [ 300/2502 ( 12%)]  Loss:  6.767949 (6.8536)  Time: 5.387s,   95.05/s  (5.194s,   98.57/s)  LR: 1.120e-04  Data: 3.843 (3.630)
Train: 0 [ 350/2502 ( 14%)]  Loss:  6.792294 (6.8460)  Time: 5.275s,   97.06/s  (5.215s,   98.17/s)  LR: 1.140e-04  Data: 3.388 (3.636)
Train: 0 [ 400/2502 ( 16%)]  Loss:  6.753360 (6.8357)  Time: 5.287s,   96.84/s  (5.231s,   97.87/s)  LR: 1.160e-04  Data: 3.604 (3.633)
Train: 0 [ 450/2502 ( 18%)]  Loss:  6.752474 (6.8274)  Time: 5.653s,   90.57/s  (5.248s,   97.56/s)  LR: 1.180e-04  Data: 3.432 (3.630)
Train: 0 [ 500/2502 ( 20%)]  Loss:  6.716499 (6.8173)  Time: 5.618s,   91.14/s  (5.277s,   97.03/s)  LR: 1.200e-04  Data: 3.594 (3.634)
Train: 0 [ 550/2502 ( 22%)]  Loss:  6.676495 (6.8056)  Time: 5.475s,   93.51/s  (5.294s,   96.71/s)  LR: 1.220e-04  Data: 3.251 (3.631)
Train: 0 [ 600/2502 ( 24%)]  Loss:  6.681392 (6.7960)  Time: 5.625s,   91.02/s  (5.314s,   96.36/s)  LR: 1.240e-04  Data: 3.918 (3.638)
Train: 0 [ 650/2502 ( 26%)]  Loss:  6.687934 (6.7883)  Time: 5.334s,   95.98/s  (5.329s,   96.08/s)  LR: 1.260e-04  Data: 3.327 (3.639)
Train: 0 [ 700/2502 ( 28%)]  Loss:  6.642119 (6.7785)  Time: 5.427s,   94.34/s  (5.348s,   95.73/s)  LR: 1.280e-04  Data: 3.728 (3.639)
Train: 0 [ 750/2502 ( 30%)]  Loss:  6.592741 (6.7669)  Time: 5.636s,   90.85/s  (5.358s,   95.57/s)  LR: 1.300e-04  Data: 3.099 (3.638)
Train: 0 [ 800/2502 ( 32%)]  Loss:  6.591680 (6.7566)  Time: 5.776s,   88.64/s  (5.365s,   95.42/s)  LR: 1.320e-04  Data: 3.646 (3.641)
Train: 0 [ 850/2502 ( 34%)]  Loss:  6.581404 (6.7469)  Time: 5.426s,   94.36/s  (5.363s,   95.47/s)  LR: 1.340e-04  Data: 3.480 (3.640)
Train: 0 [ 900/2502 ( 36%)]  Loss:  6.543271 (6.7362)  Time: 5.107s,  100.25/s  (5.362s,   95.49/s)  LR: 1.360e-04  Data: 3.518 (3.646)
Train: 0 [ 950/2502 ( 38%)]  Loss:  6.598884 (6.7293)  Time: 6.106s,   83.85/s  (5.360s,   95.53/s)  LR: 1.380e-04  Data: 3.345 (3.644)
Train: 0 [1000/2502 ( 40%)]  Loss:  6.487736 (6.7178)  Time: 5.628s,   90.98/s  (5.361s,   95.51/s)  LR: 1.400e-04  Data: 3.730 (3.645)
Train: 0 [1050/2502 ( 42%)]  Loss:  6.417177 (6.7041)  Time: 5.211s,   98.26/s  (5.366s,   95.41/s)  LR: 1.420e-04  Data: 3.563 (3.648)
Train: 0 [1100/2502 ( 44%)]  Loss:  6.515212 (6.6959)  Time: 5.798s,   88.31/s  (5.372s,   95.31/s)  LR: 1.440e-04  Data: 3.602 (3.651)
Train: 0 [1150/2502 ( 46%)]  Loss:  6.462124 (6.6862)  Time: 5.269s,   97.17/s  (5.375s,   95.25/s)  LR: 1.460e-04  Data: 3.569 (3.651)
Train: 0 [1200/2502 ( 48%)]  Loss:  6.375228 (6.6737)  Time: 5.449s,   93.96/s  (5.375s,   95.25/s)  LR: 1.480e-04  Data: 3.726 (3.648)
Train: 0 [1250/2502 ( 50%)]  Loss:  6.410439 (6.6636)  Time: 5.134s,   99.72/s  (5.377s,   95.21/s)  LR: 1.500e-04  Data: 3.048 (3.648)
Train: 0 [1300/2502 ( 52%)]  Loss:  6.441093 (6.6554)  Time: 5.235s,   97.81/s  (5.361s,   95.50/s)  LR: 1.520e-04  Data: 3.388 (3.631)
Train: 0 [1350/2502 ( 54%)]  Loss:  6.384794 (6.6457)  Time: 4.880s,  104.91/s  (5.344s,   95.81/s)  LR: 1.540e-04  Data: 3.035 (3.612)
Train: 0 [1400/2502 ( 56%)]  Loss:  6.389099 (6.6369)  Time: 4.925s,  103.95/s  (5.329s,   96.08/s)  LR: 1.560e-04  Data: 3.029 (3.596)
Train: 0 [1450/2502 ( 58%)]  Loss:  6.401996 (6.6290)  Time: 5.140s,   99.61/s  (5.315s,   96.34/s)  LR: 1.580e-04  Data: 3.180 (3.582)
Train: 0 [1500/2502 ( 60%)]  Loss:  6.296628 (6.6183)  Time: 4.727s,  108.32/s  (5.299s,   96.62/s)  LR: 1.600e-04  Data: 2.847 (3.566)
Train: 0 [1550/2502 ( 62%)]  Loss:  6.366917 (6.6105)  Time: 4.585s,  111.68/s  (5.289s,   96.81/s)  LR: 1.620e-04  Data: 3.110 (3.551)
Train: 0 [1600/2502 ( 64%)]  Loss:  6.323268 (6.6017)  Time: 4.964s,  103.15/s  (5.278s,   97.01/s)  LR: 1.639e-04  Data: 2.855 (3.537)
Train: 0 [1650/2502 ( 66%)]  Loss:  6.308566 (6.5931)  Time: 5.270s,   97.16/s  (5.267s,   97.21/s)  LR: 1.659e-04  Data: 2.943 (3.524)
Train: 0 [1700/2502 ( 68%)]  Loss:  6.315619 (6.5852)  Time: 4.834s,  105.91/s  (5.257s,   97.40/s)  LR: 1.679e-04  Data: 3.135 (3.511)
Train: 0 [1750/2502 ( 70%)]  Loss:  6.281713 (6.5768)  Time: 5.117s,  100.06/s  (5.246s,   97.59/s)  LR: 1.699e-04  Data: 2.887 (3.500)
Train: 0 [1800/2502 ( 72%)]  Loss:  6.242766 (6.5677)  Time: 5.069s,  101.01/s  (5.237s,   97.77/s)  LR: 1.719e-04  Data: 2.942 (3.491)
Train: 0 [1850/2502 ( 74%)]  Loss:  6.261586 (6.5597)  Time: 4.658s,  109.92/s  (5.228s,   97.93/s)  LR: 1.739e-04  Data: 3.116 (3.482)
Train: 0 [1900/2502 ( 76%)]  Loss:  6.301365 (6.5531)  Time: 4.752s,  107.74/s  (5.220s,   98.09/s)  LR: 1.759e-04  Data: 2.924 (3.473)
Train: 0 [1950/2502 ( 78%)]  Loss:  6.296889 (6.5467)  Time: 4.904s,  104.40/s  (5.212s,   98.24/s)  LR: 1.779e-04  Data: 3.272 (3.464)
Train: 0 [2000/2502 ( 80%)]  Loss:  6.161811 (6.5373)  Time: 5.365s,   95.43/s  (5.205s,   98.37/s)  LR: 1.799e-04  Data: 3.456 (3.455)
Train: 0 [2050/2502 ( 82%)]  Loss:  6.197736 (6.5292)  Time: 5.593s,   91.55/s  (5.198s,   98.50/s)  LR: 1.819e-04  Data: 2.752 (3.447)
Train: 0 [2100/2502 ( 84%)]  Loss:  6.223721 (6.5221)  Time: 5.047s,  101.44/s  (5.191s,   98.64/s)  LR: 1.839e-04  Data: 3.068 (3.439)
Train: 0 [2150/2502 ( 86%)]  Loss:  6.164083 (6.5139)  Time: 4.865s,  105.25/s  (5.184s,   98.77/s)  LR: 1.859e-04  Data: 3.213 (3.433)
Train: 0 [2200/2502 ( 88%)]  Loss:  6.132302 (6.5055)  Time: 5.016s,  102.07/s  (5.178s,   98.89/s)  LR: 1.879e-04  Data: 3.481 (3.426)
Train: 0 [2250/2502 ( 90%)]  Loss:  6.123135 (6.4972)  Time: 5.186s,   98.72/s  (5.171s,   99.00/s)  LR: 1.899e-04  Data: 3.321 (3.420)
Train: 0 [2300/2502 ( 92%)]  Loss:  6.143147 (6.4896)  Time: 5.016s,  102.07/s  (5.165s,   99.13/s)  LR: 1.919e-04  Data: 3.404 (3.414)
Train: 0 [2350/2502 ( 94%)]  Loss:  6.144609 (6.4824)  Time: 5.160s,   99.22/s  (5.160s,   99.23/s)  LR: 1.939e-04  Data: 3.748 (3.408)
Train: 0 [2400/2502 ( 96%)]  Loss:  6.094842 (6.4745)  Time: 5.143s,   99.55/s  (5.155s,   99.31/s)  LR: 1.959e-04  Data: 3.683 (3.402)
Train: 0 [2450/2502 ( 98%)]  Loss:  6.099330 (6.4670)  Time: 4.692s,  109.13/s  (5.150s,   99.42/s)  LR: 1.979e-04  Data: 3.374 (3.397)
Train: 0 [2500/2502 (100%)]  Loss:  6.092171 (6.4597)  Time: 8.858s,   57.80/s  (5.149s,   99.44/s)  LR: 1.999e-04  Data: 6.941 (3.395)
Train: 0 [2501/2502 (100%)]  Loss:  6.098210 (6.4527)  Time: 8.424s,   60.78/s  (5.150s,   99.41/s)  LR: 2.000e-04  Data: 3.549 (3.395)
Current checkpoints:
 ('/home/hp190122/u01961/working/deepL/transformer/timm_main_sora/checkpoint/tiny/fdb1k/pre_training/pretrain_deit_tiny_fractal1k_lr6.0e-4_epochs100_bs512_cpu/checkpoint-0.pth.tar', 6.4527182303942165)
Train: 1 [   0/2502 (  0%)]  Loss:  6.014967 (6.0150)  Time: 5.244s,   97.64/s  (5.244s,   97.64/s)  LR: 2.000e-04  Data: 3.984 (3.984)
Train: 1 [  50/2502 (  2%)]  Loss:  6.078443 (6.0467)  Time: 4.868s,  105.17/s  (5.113s,  100.14/s)  LR: 2.020e-04  Data: 3.416 (3.536)
Train: 1 [ 100/2502 (  4%)]  Loss:  6.067763 (6.0537)  Time: 4.958s,  103.26/s  (5.103s,  100.33/s)  LR: 2.040e-04  Data: 3.331 (3.547)
Train: 1 [ 150/2502 (  6%)]  Loss:  6.107190 (6.0671)  Time: 5.146s,   99.49/s  (5.119s,  100.02/s)  LR: 2.060e-04  Data: 3.514 (3.575)
Train: 1 [ 200/2502 (  8%)]  Loss:  6.183640 (6.0904)  Time: 5.325s,   96.15/s  (5.123s,   99.95/s)  LR: 2.080e-04  Data: 3.479 (3.580)
Train: 1 [ 250/2502 ( 10%)]  Loss:  5.985978 (6.0730)  Time: 5.009s,  102.22/s  (5.115s,  100.10/s)  LR: 2.100e-04  Data: 3.782 (3.564)
Train: 1 [ 300/2502 ( 12%)]  Loss:  6.074217 (6.0732)  Time: 6.282s,   81.50/s  (5.111s,  100.18/s)  LR: 2.120e-04  Data: 3.554 (3.559)
Train: 1 [ 350/2502 ( 14%)]  Loss:  6.121870 (6.0793)  Time: 4.828s,  106.05/s  (5.119s,  100.03/s)  LR: 2.140e-04  Data: 3.376 (3.568)
Train: 1 [ 400/2502 ( 16%)]  Loss:  6.016573 (6.0723)  Time: 5.092s,  100.55/s  (5.123s,   99.94/s)  LR: 2.160e-04  Data: 3.484 (3.573)
Train: 1 [ 450/2502 ( 18%)]  Loss:  6.019719 (6.0670)  Time: 5.225s,   97.98/s  (5.133s,   99.76/s)  LR: 2.180e-04  Data: 3.477 (3.581)
Train: 1 [ 500/2502 ( 20%)]  Loss:  6.036826 (6.0643)  Time: 5.216s,   98.17/s  (5.137s,   99.66/s)  LR: 2.200e-04  Data: 3.886 (3.586)
Train: 1 [ 550/2502 ( 22%)]  Loss:  5.992506 (6.0583)  Time: 5.239s,   97.74/s  (5.142s,   99.58/s)  LR: 2.220e-04  Data: 3.850 (3.592)
Train: 1 [ 600/2502 ( 24%)]  Loss:  5.872744 (6.0440)  Time: 5.448s,   93.98/s  (5.143s,   99.56/s)  LR: 2.240e-04  Data: 3.626 (3.593)
Train: 1 [ 650/2502 ( 26%)]  Loss:  5.946873 (6.0371)  Time: 4.869s,  105.16/s  (5.145s,   99.51/s)  LR: 2.260e-04  Data: 3.551 (3.592)
Train: 1 [ 700/2502 ( 28%)]  Loss:  5.986779 (6.0337)  Time: 5.114s,  100.11/s  (5.147s,   99.47/s)  LR: 2.280e-04  Data: 3.677 (3.594)
Train: 1 [ 750/2502 ( 30%)]  Loss:  5.926542 (6.0270)  Time: 5.098s,  100.43/s  (5.152s,   99.38/s)  LR: 2.300e-04  Data: 3.290 (3.591)
Train: 1 [ 800/2502 ( 32%)]  Loss:  5.983600 (6.0245)  Time: 5.356s,   95.60/s  (5.158s,   99.26/s)  LR: 2.320e-04  Data: 3.644 (3.594)
Train: 1 [ 850/2502 ( 34%)]  Loss:  5.924170 (6.0189)  Time: 5.413s,   94.58/s  (5.165s,   99.12/s)  LR: 2.340e-04  Data: 3.697 (3.594)
Train: 1 [ 900/2502 ( 36%)]  Loss:  5.963635 (6.0160)  Time: 5.176s,   98.91/s  (5.172s,   98.99/s)  LR: 2.360e-04  Data: 3.336 (3.596)
